%\VignetteIndexEntry{BeadArrayUseCases Vignette}
%\VignetteDepends{}
%\VignetteKeywords{Illumina Microarray Expression}
%\VignettePackage{BeadArrayUseCases}

\documentclass[a4paper,11pt]{article}

\textwidth=6.2in
\textheight=8.5in
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in
\parindent=0pt


\usepackage{amsthm,ragged2e,marvosym,wasysym}
\usepackage{mls40Sweave}
\usepackage[utf8]{inputenc}
\usepackage{sidecap}

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textsf{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}

\SweaveOpts{eval=FALSE, keep.source=FALSE, results=hide}

\title{ChIP-seq practical}


\author{Tom Carroll and Shamith Samarajiwa}


\renewcommand\labelenumi{\textbf{Exercise \theenumi}}



\newtheoremstyle{labexc}%
{9pt}{12pt}%      space above and below
{\sffamily\RaggedRight}%              body style
{0pt}%       heading indent amount
{\sffamily\bfseries}{:}% heading font and punctuation after it
{ }%         space after heading is a new line
{}%          head spec (empty = same as 'plain' style)

\newtheoremstyle{myplain}%
{9pt}{9pt}%      space above and below
{\RaggedRight}%              body style
{0pt}%       heading indent amount
{\sffamily\bfseries}{}% heading font and punctuation after it
{ }%         space after heading is a new line
{}%          head spec (empty = same as 'plain' style)

\newtheoremstyle{mywarning}%
{9pt}{9pt}%      space above and below
{\itshape\RaggedRight}%              body style
{0pt}%       heading indent amount
{\sffamily\bfseries}{}% heading font and punctuation after it
{ }%         space after heading is a new line
{}%          head spec (empty = same as 'plain' style)

\theoremstyle{myplain} \newtheorem*{textinfo}{\large\Info}
\theoremstyle{labexc} \newtheorem*{exc}{Use Case}
\theoremstyle{mywarning} \newtheorem*{notebell}{\Large\bell}
\theoremstyle{myplain} \newtheorem*{noteright}{\Large\Pointinghand}
\theoremstyle{myplain} \newtheorem*{textstop}{\Large\Stopsign}
\theoremstyle{myplain} \newtheorem*{commentary}{\Large\CheckedBox}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle

<<setWidth, eval=TRUE, echo=FALSE>>=
options(width=60);
@


\section*{Introduction}
 This practical aims to give an introduction to analysing ChIP-seq data in R. This will include loading aligned read data and peak calls
 from ChIP-seq data into the R enviroment, performing quality control, predicting fragment length and identifying, visualising and annotating
 peak sets.

 All code assumes that we are starting with indexed BAM files, BigWig signal graphs and Macs peak calls. Although these
 are the main formats encountered in most HTSequencing data R provides several additional functions to deal with most data types.
 BAM files are compressed aligned and unaligned reads and contain important information about the alignment steps within the header as well as convenient ways to randomly access
 parts of the file based on genomic coordinates when indexed( Index files typically BAM file name with extension '.bai')

 To get started lets first load all the libraries
<<>>=
library(chipseq)
library(GenomicRanges)
library(htSeqTools)
library(rtracklayer)
library(spp)
library(limma)
library(ChIPpeakAnno)
@

% very important to use option [fragile] for frames containing code output!

\section*{Working with Aligned Data}


All files needed within this library are contained within the directory '/Data\_For\_ChIP\_Practical/' including BAMs(.bam), Bai(.bam.bai), BigWigs ('bw)
and Macs Peaks (.bed).
Firstly we can create a BamFileList object for use within the R environment. Notice that many of the packages assuming the naming convention for BAM indicies to
be '.bam.bai' although different names may be explicitly passed to these functions.


\begin{exc}
Organising the data
\end{exc}
<<>>=
ChIPDirectory <- file.path(getwd(),"/Data_For_ChIP_Practical/")
BamFiles <- dir(ChIPDirectory,pattern="*.bam$",full.name=T)

##Make the names a little tidier
names(BamFiles) <- gsub("Chr1|\\.bam","",basename(BamFiles))
##Put into a BamFileList for use later on.
BamsForAnalysis <- BamFileList(BamFiles)
@


Now that we have all the BAM files in a convenient list lets have a look at the information within its header using scanBamHeader from Rsamtools.
scanBamHeader requires a path to the BAM file which we can get for every BAMFile in the BamFileList using path().
Lets look at the header of one file and see what information we have inside it.
\begin{exc}
Accessing information in the BAM header
\end{exc}
<<>>=
print(path(BamsForAnalysis))
samHeader <- scanBamHeader(path(BamsForAnalysis["TF_1"]))
str(samHeader,max.level=1)
@

We can see from parsing the header we have a list containing two components. The first is 'targets' which contains a list of chromosomes used in the alignment. The
second is 'text' which contains information including the species and alignment method used.

\begin{exc}
More accessing information in the BAM header
\end{exc}
<<>>=
print(samHeader[[1]]$targets)
print(samHeader[[1]]$text)
# Sort Order 
print(samHeader[[1]]$text["@HD"])
# Program used in alignment
print(samHeader[[1]]$text["@PG"])
# Species
print(samHeader[[1]]$text["@CO"])
@

Since BAM files can contain a lot of information and are typically large it is often helpful to deal with a chromosome at a time for QC and analysis.
The BAM files we are using today are already filtered to chr1 but in other instances we can select only a portion of genome using ScanBamParam.

ScanBamParam accepts two types of filtering options. The first is through scanBamFlag which select the type of reads and the second specifies the 
genomic regions of interest though a GRanges object.
In order to specify the exact coordinates of chr1 we can extract its length from the header
\begin{exc}
Select Chromosome 1 coordinates from header.
\end{exc}
<<>>=
AllSeqnames <- samHeader[[1]]$targets
LongestChr <- AllSeqnames[names(AllSeqnames) == "chr1"]
LCName <- names(LongestChr)
LCLength <- unique(LongestChr)
@
Once we have its length we can specify the area where we want reads to be drawn from. In this instance the whole of chr1. We could at this point filter out duplicates
or other potential artifacts. Have a look at the options in scanBamFlag for more details.
\begin{exc}
Set genomic region and flags to import from BAM
\end{exc}
<<>>=

?scanBamFlag

which <- GRanges(seqnames=LCName,ranges=IRanges(1,LCLength))
param <- ScanBamParam(which=which)
param
@
Now we have selected an area of interest, we can read in reads for this area using readGappedAlignments and specifying the BAM file
from our BAM List. Once we have done this we will get an idea of our read length from the GRanges object. Note the inclusion of an averaging
step incase some reads have been trimmed.
\begin{exc}
Read in data from just Chromsome 1 and get read length
\end{exc}
<<>>=

BamBnd <-  readGappedAlignments(path(BamsForAnalysis["TF_1"]),param=param)
GrangesAlign <- granges(BamBnd)
seqlevels(GrangesAlign) <- "chr1"
ReadLength <- round(median(width(GrangesAlign[1:1000])))
@

Now we have our aligned data take the time to look at information containined within the GRanges object. This includes read positions and strand.
<<>>=

GrangesAlign[1:5]
@
In this next part we can look at area of interest (a known binding event for chip-PCR). Here we want to demonstrate the strand shift seen in earlier 
presentation too so we can separate the strands using GRanges accessor 'strand'


<<>>=

AlignReadEnds_Pos <- GrangesAlign[strand(GrangesAlign) == "+"]
AlignReadEnds_Neg <- GrangesAlign[strand(GrangesAlign) == "-"]
@
Now we have extracted reads mapping to both positive and negative strands we can use these alongside all reads to generate profiles of depth of reads
along a genome. To do this we create a run length encoded list (RLEList) object containing a compressed vector of the depths of reads at all positions
in a chromosome with different chromosomes as different elements in the list. The 'coverage' function allows us to turn the reads (or any other GRanges) 
into such a list of coverage vectors
Have a look at the resulting coverage vectors. The run length encoding allows long stretches of genomic locations at the same depths to be compressed and
so saves space when creating such potentially large vectors.

\begin{exc}
Coverage vectors from GRanges object and plotting strand shift.
\end{exc}
<<>>=
 
All_Coverage <- coverage(GrangesAlign)
Pos_Coverage <- coverage(AlignReadEnds_Pos)
Neg_Coverage <- coverage(AlignReadEnds_Neg)

print(All_Coverage)
@
Once we have created the coverage vectors we can select portions of them as if they were a standard R numeric vector. Here we will extract the region of known
binding at chr1:211646604-211649812. For visualisation we run a smoothing mean function over the vector before plotting the result to demonstrate the desplacement 
between strands around a peak. We can also measure the distance between the two peaks on either strand to get an idea of the shift.
<<>>=
 
Range <- c(211646604:211649812)
Graph_RegionOfInterest_Pos <- Pos_Coverage$chr1[Range]
Graph_RegionOfInterest_Neg <- Neg_Coverage$chr1[Range]
smoothedNegCoverage <- caTools::runmean(as.vector(Graph_RegionOfInterest_Neg),20)
smoothedPosCoverage <- caTools::runmean(as.vector(Graph_RegionOfInterest_Pos),20)

png("NoReadExtension.png")
plot(smoothedNegCoverage,col="red",type="l",ylab="Smoothed Read Depth 20bp Window",xlab="bp from range start")
lines(smoothedPosCoverage,col="green",type="l")
dev.off()
which.max(smoothedNegCoverage) - which.max(smoothedPosCoverage)
@

This image illustrating the shift between reads on the positive and negative strand demonstrates the need for correcting for this bias.
To estimate the fragment length we will make use of two R packages 'chipseq' and 'SPP'.
Firstly we will use the reasonably fast method of cross-coverage from the 'chipseq' packages. This method shifts the reads from opposite strands and measures
the proportion of genome covered after each shift relative to the proportion of genome covered without a shift.
Once we have estimated the fragment lengths for each chromosome we then take a median value and extend all reads to thsi length dependent on their strand
and replot the result around our region of interest.
\begin{exc}
Estimating Fragment length from cross-coverage
\end{exc}
<<>>=
 
fraglenCov <- median(estimate.mean.fraglen(c(AlignReadEnds_Pos,AlignReadEnds_Neg),method="coverage"))
ExtendedReads <- resize(GrangesAlign,fraglenCov,fix="start")
ExtendedReads_Pos <- ExtendedReads[strand(ExtendedReads) == "+"]
ExtendedReads_Neg <- ExtendedReads[strand(ExtendedReads) == "-"]


Graph_RegionOfInterest_Pos <- coverage(ExtendedReads_Pos)$chr1[Range]
Graph_RegionOfInterest_Neg <- coverage(ExtendedReads_Neg)$chr1[Range]
smoothedNegCoverage <- caTools::runmean(as.vector(Graph_RegionOfInterest_Neg),20)
smoothedPosCoverage <- caTools::runmean(as.vector(Graph_RegionOfInterest_Pos),20)

png("Cross_Coverage_ReadExtension.png")
plot(smoothedNegCoverage,col="red",type="l",ylab="Smoothed Read Depth 20bp Window",xlab="bp from range start")
lines(smoothedPosCoverage,col="green",type="l")
dev.off()
which.max(smoothedNegCoverage) - which.max(smoothedPosCoverage)

@


An alternative approach to estimating fragment length is implemented in the SPP library used within the Encode and modEncode consortiums and this method
is the basis for the Normalised Cross Correlation and Relative Cross Correlation coefficient. This method, like that of cross-coverage, shifts the reads on
the positive strand towards the negative strand and measures the degree of correlation between reads in the positive and negative strand after each successive shift.
The shift at which the highest degree of correlation is observed between the strands is the fragment length estimate as described by Park et al. 
This library is outside of Bioconductor and so has it's own methods for reading in aligned data and manipulating the resulting R objects.
Have a look at the results from 'get.binding.characteristics'. "crosscorr\$cross.correlation" contains information on the shifts with 
"crosscorr\$cross.correlation\$y" containing the correlation values and "crosscorr\$cross.correlation\$x" containing the shifts.
\begin{exc}
Estimating Fragment length from cross-correlation
\end{exc}
<<>>=


library(spp)
readLength <- ReadLength

chip.data <-  read.bam.tags(path(BamsForAnalysis)[2])
binding.characteristics <- get.binding.characteristics(chip.data,srange=c(0,400),bin=2,accept.all.tags=T,remove.tag.anomalies = F)
crosscorr <- binding.characteristics


print(crosscorr$cross.correlation)
@
Work by Anshul Kudaje on estimating fragment length has refined the identification of fragment length from cross correlation .
In the next part of the code, adapted from Kudaje's ccQualityQuantrol package, we will smooth the correlation along shifts and exclude the peak in correlation seen at the read length. 
A peak at the read length is due to positions in the genome being non-uniquely mappable. An aligner often wont place a read at these positions
or in the case of bwa (remember we saw this in the header?) place randomly among the mappable positions for a read. When considering just the first position of read
this leads to gaps or dips in the coverage across a genome for the positive and negative strand seperated by the read length and so a peak in cross correlation at the readlength.
\begin{exc}
Cross-correlation and the Normalised/Relative cross-correlation statistics
\end{exc}
<<>>=


cc <- crosscorr$cross.correlation
MinY <- which.min(crosscorr$cross.correlation$y)
crosscorr$min.cc <- crosscorr$cross.correlation[MinY, ]
cat("Minimum cross-correlation value", crosscorr$min.cc$y,"\n",file=stdout())
cat("Minimum cross-correlation shift", crosscorr$min.cc$x,"\n",file=stdout())
sbw <- 2*floor(ceiling(5/2) / 2) + 1
cc$y <- runmean(cc$y,sbw,alg="fast")

bw <- ceiling(2/2)
peakidx <- (diff(cc$y,bw)>=0)
peakidx <- diff(peakidx,bw)
peakidx <- which(peakidx==-1) + bw

ShiftRangeExcludeMin <- 10
ShiftRangeExcludeMax <- readLength+10
peakidx <- peakidx[(cc$x[peakidx] < ShiftRangeExcludeMin) | (cc$x[peakidx] > ShiftRangeExcludeMax)]
cc <- cc[peakidx,]


maxpeakidx <- which.max(cc$y)
maxpeakshift <- cc$x[maxpeakidx]
maxpeakval <- cc$y[maxpeakidx]
peakidx <-which((cc$y >= 0.9*maxpeakval) & (cc$x >= maxpeakshift))
cc <- cc[peakidx,]

sortidx <- order(cc$y,decreasing=TRUE)
sortidx <- sortidx[c(1:min(3,length(sortidx)))]
cc.peak <- cc[sortidx,]

cat("Peak strand shift",paste(cc.peak$x,collapse=","),"\n",file=stdout())

crosscorr$peak$x <- cc.peak$x[1]
crosscorr$peak$y <- cc.peak$y[1]

ph.peakidx <- which( ( crosscorr$cross.correlation$x >= ( readLength - round(2*10) ) ) &
                     ( crosscorr$cross.correlation$x <= ( readLength + round(1.5*10) ) ) )
ph.peakidx <- ph.peakidx[ which.max(crosscorr$cross.correlation$y[ph.peakidx]) ]
crosscorr$phantom.cc <- crosscorr$cross.correlation[ph.peakidx,]
cat("Phantom peak location",crosscorr$phantom.cc$x,"\n",file=stdout())
cat("Phantom peak Correlation",crosscorr$phantom.cc$y,"\n",file=stdout())
crosscorr$phantom.coeff <- crosscorr$peak$y / crosscorr$phantom.cc$y
crosscorr$phantom.coeff <- crosscorr$peak$y / crosscorr$min.cc$y
cat("Normalized cross-correlation coefficient (NCCC)",crosscorr$phantom.coeff,"\n",file=stdout())
crosscorr$rel.phantom.coeff <- (crosscorr$peak$y - crosscorr$min.cc$y) / (crosscorr$phantom.cc$y - crosscorr$min.cc$y)
cat("Relative Cross correlation Coefficient (RCCC)",crosscorr$rel.phantom.coeff,"\n",file=stdout())
crosscorr$phantom.quality.tag <- NA
@
Higher Normalized and Relative cross-correlation coefficients indicate higher quality ChIPs.
Encode recommendations suggest samples with NSC (NCCC) < 1.05 and RSC (RCC) < 0.8 would need to be repeated.
Now we can compute the new coverage for these peaks using the fragment length predicted from cross-correlation.
<<>>=

fraglenCor <- crosscorr$peak$x

ExtendedReads <- resize(GrangesAlign,fraglenCor,fix="start")

ExtendedReads_Pos <- ExtendedReads[strand(ExtendedReads) == "+"]
##Note the strand dependent start!!!
ExtendedReads_Neg <- ExtendedReads[strand(ExtendedReads) == "-"]


Graph_RegionOfInterest_Pos <- coverage(ExtendedReads_Pos)$chr1[Range]
Graph_RegionOfInterest_Neg <- coverage(ExtendedReads_Neg)$chr1[Range]
smoothedNegCoverage <- caTools::runmean(as.vector(Graph_RegionOfInterest_Neg),20)
smoothedPosCoverage <- caTools::runmean(as.vector(Graph_RegionOfInterest_Pos),20)

which.max(smoothedNegCoverage) - which.max(smoothedPosCoverage)
@

An alternative to assessing quality using cross-correlation is presented in the htSeqTools package. Here we will 
assess the distribution of signal depths across the genome and generate the Standardised Standard Deviation (SSD), Gini and
adjusted Gini scores. 
To get the SSD score and Gini/adjusted Gini scores we can use the aligned read GRanges with the ssdCoverage and giniCoverage
functions respectively. Have a look at the results and see which of these metrics best reflects the measures of QC we have
seen so far.

\begin{exc}
Standardised Standard Deviation and Gini/adjusted Gini scores
\end{exc}
<<>>=

ssdOfSample <- ssdCoverage(GrangesAlign)
giniOfSample <- giniCoverage(GrangesAlign)
@

The htSeqTools provides a convenient way to assess the expected duplication rate and filter out reads at duplication rates
considered significantly greater than expectee. 
First we can get the number of reads across genomic locations of differing duplication rates using the tabDuplReads fucntion. This can then be used as 
input to identify the cut-off for significantly over duplicated locations. Here we generate the table, identify the significance cut-off 
and remove duplciated reads from our aligned read GRanges object. 
\begin{exc}
Calculating and filtering duplicates
\end{exc}
<<>>=

NumberOfDups <- tabDuplReads(RangedData(GrangesAlign))
CutOffs <- fdrEnrichedCounts(NumberOfDups,use=1:10,components=0,mc.cores=1)
DupFilt <- filterDuplReads(RangedData(GrangesAlign))
NumberOfDupsAfterEnrichmentTest <- tabDuplReads(DupFilt)
@

Now if you have time try calculating NSC (NCCC), RSC (RCCC), SSD, Gini and adjusted Gini for other BAM files. Which metrics agree? Is there a sample you expect to be of lower quality?


\section*{Working with Peaks in R}

In this section, we will introduce you to working with the results of peak calling within R using the GenomicRanges, Rsamtools and chipPeakAnno packages.
The peak files used in this section were generated from our BAM sample and input files using the popular peak caller MACS. This peak caller identifies
genomic windows which are enriched for sample reads when compared to input as well as global expected rate of reads in that window.
First we will read in a sample MACS output into a GRanges object.

\begin{exc}
Read a MACs file
\end{exc}
<<>>=

TestMacsFile <- read.delim(file.path(getwd(),"/Data_For_ChIP_Practical/TF_1_peaks.bed"),sep="\t",header=F)
print(TestMacsFile[1:3,])
@
We can see from the first few lines that the MACS output is very similar to standard BED6 format of Chromosome, Start position, End position, Name, Strand and Score.
Since in peak calling chIP-seq is often unstranded MACS output has no strand information but contains all other information we need to make a GRanges object.
The chromosome, strand, start and end positions are all which is need for a GRanges object and all other information is stored in the elementMetadata slots. Since 
we have no strand information we will include * for the strand information to mark it is unknown.
\begin{exc}
Calculating and filtering duplicates
\end{exc}
<<>>=

TestBed <- GRanges(seqnames=as.vector(TestMacsFile[,1]),IRanges(start=as.numeric(as.vector(TestMacsFile[,2])),end=as.numeric(as.vector(TestMacsFile[,3]))),strand=rep("*",nrow(TestMacsFile)))
elementMetadata(TestBed) <- TestMacsFile[,-c(1:3)]
colnames(elementMetadata(TestBed)) <- c("Peak_ID","Score")

TestBed[1:10]
@

Once we have the peaks in a GRanges we can then perform the GRanges methods seeen earlier in the course. Take the time to apply a few methods to this GRanges object to get an idea of 
what methods are available.
<<>>=

length(TestBed)
width(TestBed)
?GRanges
@
To read in a number of peak files I give you two convenience functions of BedToGRanges and MakeConsensusSet. Copy these into your R session. 
\begin{exc}
Convenience functions for MACs files
\end{exc}
<<>>=

Bed2GRanges <- function(BedFileName){
    BedFile <- read.delim(BedFileName,sep="\t",header=F)
      StartPos <- 2
      EndPos <- 3
      ChrPos <- 1
      TempRanges_Bed <- GRanges(seqnames=as.vector(BedFile[,ChrPos]),IRanges(start=as.numeric(as.vector(BedFile[,StartPos])),end=as.numeric(as.vector(BedFile[,EndPos]))),strand=rep("*",nrow(BedFile)))
      if(ncol(BedFile) > 3){
		elementMetadata(TempRanges_Bed) <- BedFile[,-c(ChrPos,StartPos,EndPos)]
		colnames(elementMetadata(TempRanges_Bed)) <- c("Peak_ID","Score")
	}
      TempRanges_Bed
}

MakeConsensusSet <- function(PeakFileList){

  for(i in 1:length(PeakFileList)){
    if(i == 1){
      ToMerge <- PeakFileList[[i]]
    }else{
      ToMerge <- c(ToMerge,PeakFileList[[i]])

    }
  }
  NonOverlappingSet <- reduce(ToMerge)
  Temp <- do.call(cbind,lapply(PeakFileList,function(x)countOverlaps(NonOverlappingSet,x)))
  Temp[Temp > 1] <- 1
  elementMetadata(NonOverlappingSet) <- Temp
  return(NonOverlappingSet)
}
@
These two functions will allow us to read in all MACS peaks from a directory and to find the co-occurence of peaks within this directory.
Once we have the peaks in GRanges objects we can quickly extract common sets of peaks.
\begin{exc}
Reading in peaks in a directory
\end{exc}
<<>>=

All_PeakFiles <-  dir(file.path(getwd(),"/Data_For_ChIP_Practical/"),pattern="*_peaks.bed",full.names=T)

PeakFileList <- lapply(All_PeakFiles,Bed2GRanges)
names(PeakFileList) <- gsub("_peaks.bed","",basename(All_PeakFiles))


## Peaks in file 1 which overlap peaks in 2
PeakFileList[[1]][PeakFileList[[1]] %over% PeakFileList[[2]]]
## Peaks in file 2 which overlap peaks in 1
PeakFileList[[2]][PeakFileList[[2]] %over% PeakFileList[[1]]]
## Peaks unique to file 2
PeakFileList[[2]][!PeakFileList[[2]] %over% PeakFileList[[1]]]
@


A common task in any ChIP-seq analysis is to identify peaks which occur across multiple conditions i.e.Within replicates of one condition but not another.
One approach is to construct a consensus set of peaks by "reducing" all peaks to a common, non-overlapping set and then using this consensus set as the basis for further analysis.
The MakeConsensusSet function will create a set of non-overlapping peaks within which will be contained every peak from list of GRanges peaks we created. 

\begin{exc}
Defining a consensus set
\end{exc}
<<>>=

MergedPeakSets <- MakeConsensusSet(PeakFileList)

## Peaks in file 1 which overlap peaks in consensus set
PeakFileList[[1]][PeakFileList[[1]] %over% MergedPeakSets]
## Peaks in file 2 which overlap peaks in consensus set
PeakFileList[[2]][PeakFileList[[2]] %over% MergedPeakSets]
## Peaks unique to file 2
PeakFileList[[2]][!PeakFileList[[2]] %over% MergedPeakSets]
@
The metadata within the MergedPeakSets now contains information on the occurrence of peaks used to construct the consensus set.
The column named by the peak file are made of 0s and 1s representing whether that samples peaks co-occured with consensus set (1s) or not (0s). 
From this we can identify the number of peaks co-occuring under multiple different conditions and construct subsets of GRanges object with the peak sets we are most interested in. 

\begin{exc}
Co-occurence of peaks
\end{exc}
<<>>=

a <- vennCounts(as.data.frame(elementMetadata(MergedPeakSets)))
print(a)

png("Overlap_Diagram.png")
vennDiagram(a)
dev.off()

@
Now we can select peaks which occur in TF chIPs but not the co-TF chIPs as well as the set common to both TFs
\begin{exc}
Select peaks under differing ChIP conditions
\end{exc}
<<>>=

MergedPeakSets <- MergedPeakSets[seqnames(MergedPeakSets) %in% "chr1"]
seqlevels(MergedPeakSets) <- "chr1"

MergedSets_TFonly <- MergedPeakSets[elementMetadata(MergedPeakSets)$TF_1 ==1 & elementMetadata(MergedPeakSets)$TF_2 ==1 & elementMetadata(MergedPeakSets)$CoTF ==0]
MergedSets_TFandCoTF <- MergedPeakSets[elementMetadata(MergedPeakSets)$TF_1 ==1 & elementMetadata(MergedPeakSets)$TF_2 ==1 & elementMetadata(MergedPeakSets)$CoTF ==1]

length(MergedSets_TFonly)
length(MergedSets_TFandCoTF)
@
From our refined sets it may be of interest to observe the signal of these transcription factors average over the peaks.
To do this we will use the rtracklayer package to select read coverage information from a BigWig for 1000 bp windows around the
genometric centre of these peaks.
\begin{exc}
Making average signal over peak sets plots
\end{exc}
<<>>=


bwfileCoTF <- file.path(getwd(),"/Data_For_ChIP_Practical/CoTF.bw")
bwfileTF <- file.path(getwd(),"/Data_For_ChIP_Practical/TF_1.bw")

GetAverageSignalOverRanges <- function(bwfile,selection,Window){
  resizedselection <- resize(selection,fix="center",Window)
  BWSLeft <- BigWigSelection(ranges = resizedselection)
  TempLeft <- import(bwfile,which=resizedselection,selection = BWSLeft)
  CovResLeft <- rep(TempLeft$score,width(TempLeft))
  YouLeft <-   matrix(CovResLeft,ncol=(Window),byrow=T)
  LeftMatrix1 <- apply(YouLeft, MARGIN = 2, FUN = function(X) (X - min(X))/diff(range(X)))
  Leftmeans <- colMeans(YouLeft)
  return(Leftmeans)
}

SignalOverCommon_TF <- GetAverageSignalOverRanges(bwfileTF,MergedSets_TFandCoTF,1000)
SignalOverCommon_CoTF <- GetAverageSignalOverRanges(bwfileCoTF,MergedSets_TFandCoTF,1000)
SignalOverTFOnly_TF <- GetAverageSignalOverRanges(bwfileTF,MergedSets_TFonly,1000)
SignalOverTFOnly_CoTF <- GetAverageSignalOverRanges(bwfileCoTF,MergedSets_TFonly,1000)


png("TrialPlot.png")
par(mfrow=c(1,2))
plot(SignalOverCommon_TF,col="red",ylim=c(0,max(SignalOverTFOnly_CoTF,SignalOverTFOnly_TF,SignalOverCommon_CoTF,SignalOverCommon_TF)*1.2),type="l")
lines(SignalOverCommon_CoTF,col="green")
plot(SignalOverTFOnly_TF,col="red",ylim=c(0,max(SignalOverTFOnly_CoTF,SignalOverTFOnly_TF,SignalOverCommon_CoTF,SignalOverCommon_TF)*1.2),type="l")
lines(SignalOverTFOnly_CoTF,col="green")
dev.off()
@
Up to this point all peak information in our consensus set has no information on the degree of signal at each peak location.
To annotate read count to a GRanges we can use the summarizeOverlaps function with the BamFileList object we constructed at the beginning of the 
practical.
\begin{exc}
Getting read counts in peaks
\end{exc}
<<>>=

SummarisedExperimentList <- summarizeOverlaps(MergedPeakSets,BamsForAnalysis)
assays(SummarisedExperimentList)$counts[1:10,]

elementMetadata(MergedPeakSets) <- cbind(as.data.frame(elementMetadata(MergedPeakSets)),assays(SummarisedExperimentList)$counts)
@

Now we have pooled the information about our peaks sets together we can start to investigate these peaks in a more biological context.
A common task is to annotate your peaks to genes and to visualise their distances to the closest transcriptional start sites. 
The ChIPpeakAnno package provides batch annotation of the peaks identified from either ChIP-seq or ChIP-chip experiments. It includes functions to retrieve the sequences around peaks, obtain enriched Gene Ontology (GO) terms, find the nearest gene, exon, miRNA or custom features such as most conserved elements and other transcription factor binding sites supplied by users. The package leverages the biomaRt, IRanges, Biostrings, BSgenome, GO.db, multtest and stat packages.
\begin{exc}
Annotating peaks to genes and Gene Ontology
\end{exc}
<<>>=

myPeakList1 = as(MergedSets_TFandCoTF,"RangedData") #convert GRanges to RangedData format
data(TSS.human.GRCh37) # Loads gene locations for human genome hg19.
annotatedPeak = annotatePeakInBatch (myPeakList1, AnnotationData = TSS.human.GRCh37) #annotate the peaks
write.table(as.data.frame(annotatedPeak), file = "annotatedPeakList.xls", sep = "\t", row.names = FALSE) #write to spreadsheet
y = annotatedPeak$distancetoFeature[!is.na(annotatedPeak$distancetoFeature) & annotatedPeak$fromOverlappingOrNearest == "NearestStart"] #calculate distance to feature

png("DistanceToNearestTSS.png")
hist(y, xlab = "Distance To Nearest TSS") #plot distance to feature distribution
dev.off()
png("DistributionWindowAroundTSS.png")
hist(y[y<=100 | y>=-100], xlab = "Distance To Nearest TSS") #plot to show the distribution of TF binding around TSS region (-100, +100)
dev.off()

library(org.Hs.eg.db)
enrichedGO = getEnrichedGO (annotatedPeak, orgAnn = "org.Hs.eg.db", maxP =0:01, multiAdj = TRUE, minGOterm = 10, multiAdjMethod = "BH" )
annotatedBDP = peaksNearBDP(myPeakList1, AnnotationData=TSS.human.NCBI36, MaxDistance=5000,PeakLocForDistance = "middle", FeatureLocForDistance = "TSS")
@

Finally, a chIP-seq experiment wouldn't be complete without some motif analysis. Most motif tools require a fasta formatted file of sequences to test for motif enrichment.
The BSgenome Bioconductor packages provide methods to extract and write sequences underneath peaks. Here we extract sequences under our TF and will then take these results to 
the Meme-ChIP website to identify enriched motifs.
\begin{exc}
Getting sequences under peaks
\end{exc}
<<>>=

library(BSgenome.Hsapiens.UCSC.hg19)
PeakCentres <- resize(MergedSets_TFandCoTF,fix="center",150)
RangedData_PeakCentres = as(PeakCentres,"RangedData")
Temp <- getAllPeakSequence(RangedData_PeakCentres,upstream=0,downstream=0,genome = Hsapiens) # Extracts corresponding sequences from Human genome
Sequences <- Temp$sequence
names(Sequences) <- paste(Temp$space,start(Temp),end(Temp),sep="_")
SequencesAsXstring <- DNAStringSet(Sequences)
writeXStringSet(SequencesAsXstring,file="test.fasta",width=150)
@

Now go to Meme-ChIP suite website and upload the resulting file to identify the motif of our co-occurring TF within your peak set.
http://meme.nbcr.net/meme/cgi-bin/meme-chip.cgi


\end{document}
